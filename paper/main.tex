\documentclass[twocolumn,prl,superscriptaddress,nofootinbib]{revtex4-2}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{siunitx}

\definecolor{pass}{rgb}{0.2,0.6,0.2}
\definecolor{fail}{rgb}{0.8,0.2,0.2}
\newcommand{\PASS}{\textcolor{pass}{\textbf{PASS}}}
\newcommand{\FAIL}{\textcolor{fail}{\textbf{FAIL}}}
\newcommand{\Ha}{\,\text{Ha}}
\newcommand{\kcalmol}{\,\text{kcal/mol}}

\begin{document}

\title{Can AI Agents Replicate Quantum Computing Experiments?\\A Systematic Cross-Platform Study}

\author{J.~Derek Lomas}
\affiliation{Faculty of Industrial Design Engineering, Delft University of Technology, 2628 CE Delft, The Netherlands}
\affiliation{QuTech, Delft University of Technology, 2628 CJ Delft, The Netherlands}

\date{\today}

\begin{abstract}
We investigate whether AI agents can systematically replicate published quantum computing experiments.
Using Claude Opus 4.6 as an autonomous experimental agent, we attempted to reproduce results from six landmark papers spanning variational quantum eigensolvers (VQE), quantum approximate optimization (QAOA), quantum volume (QV), randomized benchmarking (RB), and utility-scale circuits.
Each experiment was executed on three superconducting quantum processors---QI Tuna-9 (9 qubits), IQM Garnet (20 qubits), and IBM Torino (133 qubits)---plus an ideal emulator.
Of 27 claims tested, 25 (93\%) were successfully replicated, with all failures attributable to hardware noise rather than algorithmic errors.
Chemical accuracy for H$_2$ VQE was achieved on both IBM Torino (0.22\kcalmol{} with TREX) and QI Tuna-9 (0.56\kcalmol{} via a true hybrid classical-quantum optimization loop with readout error mitigation).
We certify QV${}=16$ on Tuna-9 with 100 random circuits (mean heavy output fraction 0.757, $2\sigma$ confidence), characterize all 9 qubits via randomized benchmarking (mean gate fidelity 99.55\%), and demonstrate that stacked readout error mitigation with quadratic zero-noise extrapolation achieves 2.9\,mHa average error across the H$_2$ dissociation curve---with 3 of 7 bond distances at chemical accuracy.
Cross-platform execution reveals systematic differences in noise character: Tuna-9 and Garnet exhibit dephasing noise while Torino shows depolarizing noise.
All code, data, and replication reports are available at \url{https://github.com/JDerekLomas/quantuminspire}.
\end{abstract}

\maketitle

% ===================================================================
\section{Introduction}
\label{sec:intro}
% ===================================================================

The reproducibility crisis in science is well documented~\cite{baker2016}, but quantum computing faces a particularly acute form of the problem.
Published experimental results depend on specific hardware, custom calibration procedures, and implicit knowledge about error mitigation---details that are often underspecified in papers.
As the field matures and quantum devices become more accessible through cloud platforms, the question of which published results can be independently verified becomes increasingly important.

We propose a novel approach: using an AI agent to systematically attempt replication of published quantum experiments.
The agent reads the paper, extracts the experimental protocol, generates quantum circuits, submits them to multiple hardware backends, and compares results to the published claims.
The key insight is that the \emph{failure modes}---the systematic ways in which replications deviate from published results---are themselves a valuable research contribution.
They reveal what information is missing from papers, how results depend on hardware-specific calibration, and how quantum processors compare under identical test conditions.

This work makes five contributions:
\begin{enumerate}
  \item A systematic replication of six landmark quantum computing papers across three hardware platforms, achieving a 93\% overall pass rate (25/27 claims).
  \item A cross-platform diagnostic suite that reveals distinct noise fingerprints across processors: dephasing on Tuna-9 and IQM Garnet versus depolarizing on IBM Torino.
  \item A comprehensive characterization of Tuna-9: QV${}=16$ certification (100 circuits), single-qubit randomized benchmarking on all 9 qubits (mean fidelity 99.55\%), and a stacked error mitigation ladder (Raw $\to$ REM $\to$ REM+ZNE) reducing H$_2$ VQE error from 31.8 to 2.9\,mHa.
  \item A true hybrid classical-quantum VQE loop with COBYLA optimization calling Tuna-9 hardware at each iteration, achieving chemical accuracy (0.9\,mHa) in 17 iterations.
  \item An open-source replication pipeline and dataset comprising 100+ experiment result files, 600{,}000+ measurement shots, and structured replication reports.
\end{enumerate}

% ===================================================================
\section{Methods}
\label{sec:methods}
% ===================================================================

\subsection{AI Agent Architecture}

All experimental work was performed by Claude Opus 4.6~(Anthropic), operating as an autonomous agent within the Claude Code command-line interface (CLI).
The agent had access to Python~3.12 with Qiskit~2.1.2, PennyLane~0.44, and the Quantum Inspire SDK~3.5.1, plus the \texttt{qxelarator} local emulator for noiseless simulation.

Hardware access was mediated by three Model Context Protocol (MCP) tool servers---lightweight processes that expose quantum backends as callable tools.
Each server wraps a vendor SDK: the \texttt{qi-circuits} server exposes Tuna-9 (via \texttt{RemoteBackend}) and the local emulator (via \texttt{LocalBackend}); the \texttt{ibm-quantum} server wraps \texttt{QiskitRuntimeService} for IBM Torino; and the \texttt{qrng} server provides certified quantum random numbers.
The agent calls these tools by name (e.g., \texttt{qi\_submit\_circuit}, \texttt{ibm\_submit\_circuit}) without knowledge of the underlying SDK implementation, enabling backend-agnostic circuit submission.

No human intervention occurred during circuit design, submission, or analysis; the human role was limited to selecting target papers and reviewing final reports.

\subsection{Replication Pipeline}

Each replication proceeds through five stages (Fig.~\ref{fig:pipeline}):

\begin{enumerate}
  \item \textbf{Claim extraction.} The agent reads the target paper and populates a structured claims registry (\texttt{replication\_analyzer.py}), recording for each claim: the metric type, published value and uncertainty, figure/table reference, and experimental conditions (bond distance, ansatz depth, error mitigation used).
  \item \textbf{Circuit generation.} A paper-specific replication script (e.g., \texttt{replicate\_sagastizabal.py}) implements the algorithm---deriving molecular Hamiltonians via Jordan-Wigner or Bravyi-Kitaev transformations, constructing ansatz circuits, and computing optimal variational parameters.
  \item \textbf{Emulator validation.} Circuits are first run on the noiseless emulator to confirm algorithmic correctness before consuming hardware time. All five papers reproduced exactly on the emulator.
  \item \textbf{Hardware execution.} An experiment daemon (\texttt{experiment\_daemon.py}, 2{,}700 lines) continuously polls a JSON job queue, generates native-format circuits (OpenQASM~2.0 for IBM, cQASM~3.0 for QI), submits via MCP servers, and stores raw measurement counts with SHA-256 checksums. The daemon handles state recovery (resetting stalled jobs after 15 minutes), process locking, and automatic git commits of results.
  \item \textbf{Automated comparison.} The replication analyzer loads result files, extracts metrics using claim-specific extractors (handling diverse result formats: VQE energy sweeps, QV nested dictionaries, QAOA approximation ratios), and classifies each discrepancy into one of five failure modes (described below under Failure Taxonomy).
\end{enumerate}

\begin{figure}[h]
  \centering
  \setlength{\unitlength}{1pt}
  \begin{picture}(230,38)
    \footnotesize
    \put(0,12){\framebox(42,18){\shortstack{Paper\\extract}}}
    \put(42,21){\vector(1,0){8}}
    \put(50,12){\framebox(42,18){\shortstack{Circuit\\generate}}}
    \put(92,21){\vector(1,0){8}}
    \put(100,12){\framebox(42,18){\shortstack{Emulator\\validate}}}
    \put(142,21){\vector(1,0){8}}
    \put(150,12){\framebox(42,18){\shortstack{Hardware\\execute}}}
    \put(192,21){\vector(1,0){8}}
    \put(200,12){\framebox(42,18){\shortstack{Compare\\classify}}}
  \end{picture}
  \caption{Five-stage replication pipeline. Each stage is fully automated; the daemon handles hardware submission and result collection.}
  \label{fig:pipeline}
\end{figure}

Each experiment result is stored as a self-contained JSON file with raw measurement counts, circuit description, backend metadata, timestamps, and cryptographic checksums---98 such files comprising 230{,}000+ measurement shots across all backends.

\subsection{Paper Selection}

We selected six papers spanning the major categories of NISQ-era quantum computing experiments (Table~\ref{tab:papers}).
Selection criteria were: (1)~published results on real quantum hardware, (2)~2--50 qubit circuits feasible on our hardware, (3)~sufficient protocol detail for independent replication, and (4)~diverse experiment types.

\begin{table}[b]
\caption{Target papers for replication.}
\label{tab:papers}
\begin{ruledtabular}
\begin{tabular}{lllc}
Paper & Type & Year & Claims \\
\midrule
Sagastizabal~\cite{sagastizabal2019} & VQE + error mitigation & 2019 & 4 \\
Kandala~\cite{kandala2017} & VQE (hw-efficient) & 2017 & 5 \\
Peruzzo~\cite{peruzzo2014} & VQE (original) & 2014 & 9 \\
Cross~\cite{cross2019} & QV + RB & 2019 & 3 \\
Harrigan~\cite{harrigan2021} & QAOA MaxCut & 2021 & 4 \\
Kim~\cite{kim2023} & Utility-scale circuits & 2023 & 3 \\
\midrule
& & Total: & 27* \\
\end{tabular}
\end{ruledtabular}
\end{table}

\noindent *Kim~\cite{kim2023} used a 9-qubit subset of the original 127-qubit experiment.

\subsection{Hardware Platforms}

Experiments were run on three superconducting quantum processors (Table~\ref{tab:platforms}) plus the QI \texttt{qxelarator} noiseless emulator as a reference.
All circuits were expressed in the native instruction set of each platform and submitted via cloud APIs.
No manual qubit selection or gate calibration was performed unless specified.

\begin{table}[b]
\caption{Hardware platforms used in this study.}
\label{tab:platforms}
\begin{ruledtabular}
\begin{tabular}{lcccl}
Platform & Qubits & QV & Topology & Native gates \\
\midrule
QI Tuna-9 & 9 & 16 & Tree & CZ, Ry, Rz \\
IQM Garnet & 20 & 32 & Square & prx, CZ \\
IBM Torino & 133 & 32 & Heavy-hex & CZ, SX, RZ \\
\end{tabular}
\end{ruledtabular}
\end{table}

\subsection{Claim Extraction and Evaluation}

For each paper, we extracted testable claims---quantitative statements about energies, fidelities, or algorithmic performance---and defined pass/fail criteria:
\begin{itemize}
  \item \textbf{VQE energy}: within chemical accuracy (1.6\kcalmol{} $\approx$ 0.0016\Ha) of the exact value.
  \item \textbf{Bell/GHZ fidelity}: within 5\% of emulator baseline.
  \item \textbf{Quantum volume}: heavy output fraction $> 2/3$ with $> 97.5\%$ confidence.
  \item \textbf{QAOA}: approximation ratio exceeds random assignment ($> 0.5$).
  \item \textbf{RB}: gate fidelity $> 99\%$.
\end{itemize}

\noindent
Failure classification goes beyond pass/fail.
Each discrepancy is assigned to one of five modes: \emph{noise degradation} (hardware noise exceeds signal), \emph{topology constraint} (connectivity prevents the circuit), \emph{compilation artifact} (transpiler alters effective circuit depth), \emph{calibration sensitivity} (results vary between qubit pairs or runs), or \emph{error mitigation dependency} (success requires a specific mitigation technique).
This taxonomy is applied automatically by the analyzer and refined by human review.

\subsection{Reproducing This Work}

To replicate our replication, a researcher needs: (1)~cloud accounts on IBM Quantum, Quantum Inspire, and/or IQM Resonance (all offer free tiers); (2)~Python~3.12 with the packages listed in \texttt{requirements.txt}; and (3)~the repository itself.
The minimal workflow is:
\begin{verbatim}
  git clone github.com/JDerekLomas/
    quantuminspire
  pip install -r requirements.txt
  # Run a single replication:
  python replicate_sagastizabal.py
  # Or queue experiments for the daemon:
  python agents/experiment_daemon.py
\end{verbatim}
Each replication script is self-contained: it derives the Hamiltonian, builds circuits, runs on the emulator, and saves results to \texttt{experiments/results/}.
Hardware submission requires API credentials but the emulator runs locally with no account needed.
The replication analyzer can then be run to compare any new results against published claims:
\begin{verbatim}
  python agents/replication_analyzer.py \
    --paper sagastizabal2019
\end{verbatim}
All 98 result files from this study are included in the repository, so the comparison and failure classification can be reproduced without hardware access.

% ===================================================================
\section{Cross-Platform Characterization}
\label{sec:platforms}
% ===================================================================

Before attempting paper replication, we ran a standardized diagnostic suite on all three processors to establish baseline performance.
The suite comprised Bell state preparation with three-basis tomography, GHZ state preparation at increasing qubit counts, quantum volume circuits, and single-qubit randomized benchmarking.

\subsection{Bell State Tomography}

Bell state preparation ($|\Phi^+\rangle = (|00\rangle + |11\rangle)/\sqrt{2}$) measured in the $Z$, $X$, and $Y$ bases yields three correlators whose relative magnitudes fingerprint the dominant noise channel~\cite{temme2017}.
Table~\ref{tab:bell} summarizes the results.

\begin{table}[h]
\caption{Bell state characterization across platforms. Best qubit pair used for Tuna-9 and Garnet; default transpiler placement for Torino.}
\label{tab:bell}
\begin{ruledtabular}
\begin{tabular}{lccc}
& Tuna-9 & Garnet & Torino \\
\midrule
$\langle ZZ \rangle$ & 0.871 & 0.963 & 0.729 \\
$\langle XX \rangle$ & 0.803 & 0.911 & 0.704 \\
$|\langle YY \rangle|$ & 0.792 & 0.929 & 0.675 \\
\midrule
Fidelity (direct) & 93.5\% & 98.1\% & 86.5\% \\
Noise type & Dephasing & Dephasing & Depolarizing \\
\end{tabular}
\end{ruledtabular}
\end{table}

Tuna-9 and Garnet show a clear dephasing signature: $\langle ZZ \rangle$ is significantly larger than $\langle XX \rangle$ and $|\langle YY \rangle|$, indicating that $Z$-basis correlations are better preserved than transverse correlations.
Torino, in contrast, shows all three correlators within 5\% of each other---the hallmark of depolarizing noise, where errors are equally distributed across Pauli channels (Fig.~\ref{fig:noise}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig2_noise_fingerprint.pdf}
  \caption{Noise fingerprint from Bell state tomography. Dephasing noise (Tuna-9, Garnet) shows $\langle ZZ \rangle > \langle XX \rangle \approx |\langle YY \rangle|$; depolarizing noise (Torino) shows all correlators approximately equal. Best qubit pair used for Tuna-9 and Garnet; default transpiler placement for Torino.}
  \label{fig:noise}
\end{figure}

\subsection{GHZ Scaling}

We prepared GHZ states ($|0\rangle^{\otimes n} + |1\rangle^{\otimes n})/\sqrt{2}$ for $n = 3, 5, 10, 20, 50$ qubits (hardware permitting) and measured the fidelity as the fraction of outcomes in the $\{|0\rangle^{\otimes n}, |1\rangle^{\otimes n}\}$ subspace.
Figure~\ref{fig:ghz} shows the results.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig1_ghz_scaling.pdf}
  \caption{GHZ state fidelity as a function of qubit count. Emulator (dashed) achieves 100\% at all sizes. IBM Torino's 50-qubit GHZ (8.5\% fidelity) represents the largest entangled state in this study. Per-qubit error is approximately constant ($\sim$5\%) across all circuit sizes on Torino.}
  \label{fig:ghz}
\end{figure}

\begin{table}[h]
\caption{GHZ fidelity (\%) across platforms.}
\label{tab:ghz}
\begin{ruledtabular}
\begin{tabular}{rcccc}
$n$ & Emulator & Tuna-9 & Garnet & Torino \\
\midrule
3  & 100 & 88.9 & 93.9 & 82.9 \\
5  & 100 & 83.8 & 81.8 & 76.6 \\
10 & 100 & ---  & 54.7 & 62.2 \\
20 & 100 & ---  & ---  & 34.3 \\
50 & 100 & ---  & ---  & 8.5  \\
\end{tabular}
\end{ruledtabular}
\end{table}

Remarkably, the per-qubit error rate is approximately constant across circuit sizes on IBM Torino: $\epsilon \approx 5\%$ from $n=3$ to $n=50$.
This suggests that GHZ fidelity is dominated by local errors rather than crosstalk, at least for the heavy-hex topology where linear chains avoid crowded qubit neighborhoods.

\subsection{Quantum Volume}

We measured quantum volume using the standard protocol~\cite{cross2019}: random SU(4) circuits of width $n$ and depth $n$, with 5 trials per width.
A width passes if the heavy output fraction exceeds $2/3$ with statistical significance.

\begin{table}[h]
\caption{Quantum volume results. Heavy output fraction (mean of 5 trials unless noted). $^\dagger$100 circuits with 1024 shots each; 97/100 pass individually, $2\sigma$ lower bound 0.746.}
\label{tab:qv}
\begin{ruledtabular}
\begin{tabular}{lcccc}
Width & Tuna-9 & Garnet & Torino & Pass threshold \\
\midrule
$n=2$ & 0.692 & 0.757 & 0.698 & 0.667 \\
$n=3$ & 0.821 & 0.635 & 0.736 & 0.667 \\
$n=4$ & 0.757$^\dagger$ & 0.686 & 0.706 & 0.667 \\
$n=5$ & ---   & 0.713 & 0.676 & 0.667 \\
$n=6$ & ---   & ---   & 0.602 & 0.667 \\
\midrule
QV    & 16    & 32    & 32    & \\
\end{tabular}
\end{ruledtabular}
\end{table}

Both Garnet and Torino achieve QV~$= 32$ despite Torino having $6.7\times$ more qubits (Fig.~\ref{fig:qv}).
This illustrates a known limitation of quantum volume as a metric: it measures the performance of the \emph{best} subset of qubits, not the full processor.

Notably, Tuna-9 achieves QV~$= 16$---double the initially measured QV~$= 8$ from preliminary 5-trial runs.
The QV~$= 16$ certification uses 100 random SU(4) circuits at width $n = 4$ on qubits \{4, 6, 7, 8\} (the best 4-cycle subgraph), with 1024 shots each.
The mean heavy output fraction is $0.757 \pm 0.005$, with a $2\sigma$ lower confidence bound of 0.746---well above the $2/3$ threshold.
Of 100 circuits, 97 pass individually (HOF range 0.655--0.898), demonstrating that QV~$= 16$ is robustly achieved rather than marginally scraped.
This improvement reflects the importance of qubit selection: the \{4, 6, 7, 8\} subgraph has the highest CZ fidelity and best single-qubit gate performance on the processor.

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig3_quantum_volume.pdf}
  \caption{Quantum volume comparison. Dashed line indicates the $2/3$ pass threshold. Both Garnet and Torino achieve QV~$= 32$; Tuna-9 achieves QV~$= 16$. Each point is the mean heavy output fraction over 5 random SU(4) circuit trials, except Tuna-9 at $n=4$ which uses 100 circuits.}
  \label{fig:qv}
\end{figure}

% ===================================================================
\section{Replication Results}
\label{sec:results}
% ===================================================================

Table~\ref{tab:replication} and Fig.~\ref{fig:scorecard} summarize the replication outcomes across all six papers.
Of 27 claims tested, 25 passed (93\%).
Both failures occurred on hardware platforms due to noise, not due to errors in the AI agent's circuit construction or analysis.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig4_scorecard.pdf}
  \caption{Replication scorecard across six papers and three backends. Green = PASS (claim replicated within stated criteria), red = FAIL, grey = not tested on that platform. Both failures are HeH$^+$ VQE claims where coefficient amplification makes hardware noise exceed molecular signal.}
  \label{fig:scorecard}
\end{figure*}

\begin{table*}
\caption{Replication scorecard. \PASS{} = claim replicated within stated criteria, \FAIL{} = claim not met, --- = not tested.
Superscripts: $^a$TREX mitigation, $^b$post-selection, $^c$Torino only, $^d$PS+REM hybrid, $^e$hybrid VQE+REM.}
\label{tab:replication}
\begin{ruledtabular}
\footnotesize
\begin{tabular}{llcccc}
Paper & Claim & Emulator & Tuna-9 & Garnet/Torino & All \\
\midrule
\multirow{4}{*}{Sagastizabal~\cite{sagastizabal2019}}
  & H$_2$ energy ($-1.137\Ha$) & \PASS & \PASS & \PASS$^c$ & \PASS \\
  & Sym.\ verif.\ $>2\times$ & --- & \PASS{} ($3.6\times$) & \PASS{} ($119\times$)$^a$ & \PASS \\
  & Chemical accuracy & \PASS & \PASS{} (0.56)$^e$ & \PASS{} (0.22)$^a$ & \PASS \\
  & Post-sel.\ $>$95\% & --- & \PASS{} (96\%) & --- & \PASS \\
\midrule
\multirow{5}{*}{Kandala~\cite{kandala2017}}
  & PES MAE $<1.6$\,mHa & \PASS & --- & --- & \PASS \\
  & HW-efficient ansatz & \PASS & --- & \PASS$^c$ & \PASS \\
  & Chem.\ acc.\ + mitigation & \PASS & \PASS$^d$ & \PASS$^a$ & \PASS \\
  & Multi-pair consistency & --- & \PASS{} (q[2,4], q[6,8]) & --- & \PASS \\
  & PES sweep quality & \PASS & \PASS & --- & \PASS \\
\midrule
\multirow{5}{*}{Peruzzo~\cite{peruzzo2014}}
  & HeH$^+$ energy (R=0.75) & \PASS & \PASS{} (4.44)$^d$ & \PASS{} (4.45)$^a$ & \PASS \\
  & HeH$^+$ curve MAE & \PASS & --- & \FAIL{} (4.3--7.3)$^a$ & \FAIL \\
  & HeH$^+$ chem.\ accuracy & \PASS & --- & \FAIL{} (4.31)$^a$ & \FAIL \\
  & Sym.\ verif.\ helps & \PASS & \PASS & \PASS$^c$ & \PASS \\
  & Coeff.\ amplification pred. & --- & \PASS & \PASS$^c$ & \PASS \\
\midrule
\multirow{3}{*}{Cross~\cite{cross2019}}
  & QV $\geq 8$ & \PASS & \PASS & \PASS & \PASS \\
  & QV validates correctly & \PASS & \PASS & \PASS & \PASS \\
  & RB fidelity $> 99\%$ & \PASS & \PASS{} (99.55\%) & \PASS & \PASS \\
\midrule
\multirow{4}{*}{Harrigan~\cite{harrigan2021}}
  & QAOA $>$ random & \PASS & \PASS{} (0.741) & --- & \PASS \\
  & Depth improves ratio & \PASS & \PASS & --- & \PASS \\
  & 3-regular performance & \PASS & --- & --- & \PASS \\
  & Tree subgraph & --- & \PASS & --- & \PASS \\
\midrule
\multirow{3}{*}{Kim~\cite{kim2023}}
  & Kicked Ising dynamics & \PASS & \PASS & \PASS$^c$ & \PASS \\
  & ZNE improves accuracy & \PASS{} (14.1$\times$) & \PASS{} (2.3$\times$) & \PASS{} (1.3$\times$)$^{a,c}$ & \PASS \\
  & 9-qubit scaling & \PASS & \PASS & \PASS$^c$ & \PASS \\
\midrule
\multicolumn{2}{l}{\textbf{Total}} & \textbf{26/26} & \textbf{24/26} & \textbf{20/22} & \textbf{25/27} \\
\end{tabular}
\end{ruledtabular}
\end{table*}

\subsection{VQE Replications}

The three VQE papers (Sagastizabal, Kandala, Peruzzo) represent the core of our replication effort.
On the emulator, all three reproduce perfectly: the AI agent correctly derives the molecular Hamiltonians via Jordan-Wigner transformation, constructs the appropriate ansatz circuits, and optimizes variational parameters to within $< 1\kcalmol$ of the exact (FCI) ground state energy.

On hardware, results diverge (Fig.~\ref{fig:vqe}).
H$_2$ at equilibrium bond length ($R = 0.735$\,\AA) achieves chemical accuracy on two platforms: IBM Torino achieves $-1.138\Ha$ ($0.22\kcalmol$) with TREX error mitigation~\cite{kim2023}, and Tuna-9 achieves $0.56\kcalmol$ ($0.9$\,mHa) via a true hybrid classical-quantum optimization loop with readout error mitigation.
With pre-optimized parameters, Tuna-9 achieves $0.92\kcalmol$ using hybrid post-selection + REM on qubit pair q[2,4], and $1.32\kcalmol$ on a second pair q[6,8].

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig5_vqe_h2.pdf}
  \caption{H$_2$ potential energy surface. Exact FCI values (black), emulator (grey dashed, nearly indistinguishable), Tuna-9 with post-selection (blue), and IBM Torino with TREX (green diamond, single point at $R = 0.735$\,\AA{} within the chemical accuracy band). Hardware error increases at large $R$ where entanglement gates amplify noise.}
  \label{fig:vqe}
\end{figure}

IQM Garnet, tested with the same hybrid PS+REM approach, achieves $14.26\kcalmol$---an order of magnitude worse than Tuna-9 despite having lower readout error (1.1\% vs.\ 9.2\% on Tuna-9's best qubit).
The dominant error source on Garnet is gate noise rather than readout, making confusion-matrix-based REM insufficient.
An initial attempt produced $60.34\kcalmol$ due to a CNOT decomposition error specific to IQM's native gate set: the standard textbook decomposition $\text{CNOT} = H \cdot CZ \cdot H$ fails on IQM because IQM's Hadamard gate ($\text{prx}(\pi,0) \cdot \text{prx}(\pi/2,\pi/2)$) differs from the standard $H$ by a global phase of $-i$, which becomes a \emph{relative} phase of $-1$ on $|00\rangle$ when composed as $H \cdot CZ \cdot H$.
This negates the $\langle XX \rangle$ and $\langle YY \rangle$ correlators, producing the wrong VQE energy.
The correct decomposition uses $R_y(\pi/2) \cdot CZ \cdot R_y(-\pi/2)$, yielding a $4.2\times$ error reduction.
This platform-specific gate decomposition subtlety is not documented in IQM's public materials and was discovered through systematic debugging of sign-flipped expectation values.

HeH$^+$, however, proves far more challenging.
The molecular Hamiltonian has a $|g_1|/|g_4|$ ratio of 7.8 (vs.\ 4.4 for H$_2$), meaning single-qubit $Z$ errors are amplified $\sim$20$\times$ relative to the entanglement signal.
With TREX on IBM Torino, the best result is $4.31\kcalmol$ at $R = 1.50$\,\AA---a 2.3--4.3$\times$ improvement over raw, but still 20$\times$ worse than H$_2$ TREX ($0.22\kcalmol$).
On Tuna-9 with REM+PS, HeH$^+$ achieves $4.44\kcalmol$ at $R = 0.75$\,\AA, comparable to IBM.
This coefficient amplification effect is predictive: given a Hamiltonian's $|g_1|/|g_4|$ ratio and hardware noise characterization, one can estimate whether VQE will achieve chemical accuracy without running the experiment.

\subsection{Quantum Volume and Randomized Benchmarking}

The Cross~\cite{cross2019} replication was the most straightforward.
Quantum volume is a well-defined protocol with clear pass/fail criteria, and all three processors passed at their expected levels (Table~\ref{tab:qv}).

We performed standard single-qubit Clifford randomized benchmarking on all 9 Tuna-9 qubits independently (Table~\ref{tab:rb}), using 24 Clifford gates decomposed into native Ry/Rz via ZYZ decomposition, with sequence lengths $[1, 4, 8, 16, 32, 64]$ and 5 random seeds per length.
Circuits were submitted with \texttt{compile\_stage=routing} (no server-side compilation), so the measured error per Clifford reflects actual gate performance.

\begin{table}[h]
\caption{Single-qubit randomized benchmarking on Tuna-9 (all 9 qubits, 1024 shots, 5 seeds per length).}
\label{tab:rb}
\begin{ruledtabular}
\begin{tabular}{lccc}
Qubit & Gate fidelity (\%) & EPC ($10^{-3}$) & Depol.\ param.\ $p$ \\
\midrule
q0 & 99.75 & 2.51 & 0.995 \\
q1 & 98.64 & 13.65 & 0.973 \\
q2 & 99.56 & 4.37 & 0.991 \\
q3 & 99.38 & 6.19 & 0.988 \\
q4 & 99.57 & 4.26 & 0.991 \\
q5 & 99.70 & 3.04 & 0.994 \\
q6 & 99.95 & 0.51 & 0.999 \\
q7 & 99.96 & 0.39 & 0.999 \\
q8 & 99.44 & 5.64 & 0.989 \\
\midrule
Mean & 99.55 & 4.51 & 0.992 \\
\end{tabular}
\end{ruledtabular}
\end{table}

The mean gate fidelity across all 9 qubits is 99.55\%.
The best qubits (q6, q7) achieve $> 99.95\%$ fidelity with error per Clifford below $10^{-3}$---these are the qubits used for our QV${}=16$ certification subgraph.
The worst qubit (q1) at 98.64\% is an outlier, with $3\times$ higher error than the next-worst.
Notably, the VQE qubit pair (q4, q6) both exceed 99.5\% fidelity, supporting our choice of this pair for chemistry calculations.

For comparison, IBM Torino reports 99.99\% and IQM Garnet reports $\sim$100\% single-qubit fidelity via standard RB.
However, these values are \emph{compilation artifacts}: the Qiskit and IQM transpilers aggressively simplify Clifford sequences, so the benchmarked circuit is much shorter than the logical circuit.
Tuna-9's values, derived from circuits submitted without compiler optimization, reflect actual gate performance.

\subsection{QAOA Replication}

Harrigan~\cite{harrigan2021} demonstrated QAOA for MaxCut on non-planar graphs using Google's Sycamore processor.
Our replication faced topology constraints: Tuna-9's tree connectivity (10 edges among 9 qubits) prohibits triangles, limiting us to tree-compatible subgraphs.

On the emulator, all 10 test graphs achieved approximation ratios exceeding random assignment, with 8/10 improving with increased QAOA depth---matching the paper's claims.
On Tuna-9, a 4-node tree subgraph achieved an approximation ratio of 0.534 at $p=1$, modestly above the random baseline of 0.5.
A parameter sweep over $(\gamma, \beta)$ improved this to 0.741---demonstrating that the optimization landscape is accessible but that single-point execution underperforms.

\subsection{Mitigation Technique Ranking}

We systematically compared 8 error mitigation configurations on IBM Torino for H$_2$ VQE at $R = 0.735$\,\AA{} (Table~\ref{tab:mitigation}).

\begin{table}[h]
\caption{Mitigation ladder on IBM Torino (H$_2$ VQE, $R = 0.735$\,\AA, 4096 shots). Chemical accuracy threshold: $1.6\kcalmol$.}
\label{tab:mitigation}
\begin{ruledtabular}
\begin{tabular}{lcc}
Technique & Error (\kcalmol) & Improvement \\
\midrule
TREX & \textbf{0.22} & $119\times$ \\
TREX + DD & 1.33 & $20\times$ \\
Post-selection & 1.66 & $16\times$ \\
SamplerV2 + DD + PS & 3.50 & $7\times$ \\
TREX + 16K shots & 3.77 & $7\times$ \\
TREX + DD + Twirl & 10.0 & $3\times$ \\
ZNE (linear) & 12.84 & $2\times$ \\
Raw baseline & 26.2 & $1\times$ \\
\end{tabular}
\end{ruledtabular}
\end{table}

The key finding: TREX alone achieves chemical accuracy. Adding dynamical decoupling (DD) degrades the result by $6\times$; adding gate twirling makes it $45\times$ worse. ZNE is counterproductive for this circuit depth. More shots (16K vs 4K) do not improve TREX. The intuition that ``more mitigation = better'' is wrong for shallow circuits where readout error dominates.

\subsection{Error Mitigation on Tuna-9}

On Tuna-9, we explored multiple mitigation strategies.
Post-selection alone gives $\sim$7$\kcalmol$ mean error. Readout error mitigation (REM) via confusion matrix calibration and inversion corrects measurement bias in $X/Y$ bases, reducing the mean error to 7.4\,mHa across the dissociation curve.
A hybrid PS+REM approach (post-selection on $Z$-basis, REM on $X/Y$-basis) achieves $0.92\kcalmol$ on qubit pair q[2,4] at the equilibrium distance.

The most effective strategy is stacked REM + quadratic ZNE (CZ gate folding with fold factors 1, 3, 5 and Richardson extrapolation), which achieves 2.9\,mHa average across all 7 bond distances, with 3/7 at chemical accuracy (Table~\ref{tab:tuna9_mitigation}).
Notably, linear ZNE (fold 1, 3) \emph{worsens} the result to 8.6\,mHa---the extrapolation overshoots because the noise response is sublinear in fold factor.
Quadratic extrapolation from three fold points corrects this.

\begin{table}[h]
\caption{Error mitigation ladder on Tuna-9 (H$_2$ VQE, 7 bond distances, 5 reps, 4096 shots). Chemical accuracy: 1.6\,mHa.}
\label{tab:tuna9_mitigation}
\begin{ruledtabular}
\begin{tabular}{lccc}
Method & Avg error (mHa) & Best (mHa) & Chem.\ acc. \\
\midrule
Raw & 31.8 & 20.3 & 0/7 \\
REM & 7.4 & 2.7 & 0/7 \\
REM + ZNE (linear) & 8.6 & 5.1 & 0/7 \\
\textbf{REM + ZNE (quad.)} & \textbf{2.9} & \textbf{0.4} & \textbf{3/7} \\
\end{tabular}
\end{ruledtabular}
\end{table}

The best single-point result comes from a true hybrid classical-quantum VQE loop.
COBYLA~\cite{powell1994} calls Tuna-9 hardware at each iteration (3 circuits/iteration, 4096 shots each), starting from $\alpha = -0.22$ at $R = 0.735$\,\AA.
The optimizer converges in 17 iterations (51 hardware circuits), achieving a best energy of $-1.13641\Ha$ at iteration 12---an error of \textbf{0.9\,mHa} ($0.56\kcalmol$), well within chemical accuracy and competitive with IBM Torino's TREX result ($0.22\kcalmol$).
This demonstrates that Tuna-9's noise is low enough for a classical optimizer to navigate the energy landscape using only hardware evaluations.

\subsection{Kim et al.\ (2023): Utility-Scale Circuits}

We replicated the kicked Ising dynamics from Kim et al.~\cite{kim2023} on a 9-qubit scaled-down version of the original 127-qubit experiment, testing on emulator, IBM Torino, and QI Tuna-9.
On the emulator, ZNE via gate folding provides a $14.1\times$ improvement in accuracy.
On IBM Torino with TREX (6 Trotter depths, 5 $\theta$ values), ZNE gives a $1.3\times$ improvement---modest because TREX already handles readout errors, and gate noise accumulates rapidly with circuit depth (up to 180 CZ gates at depth 10).
On Tuna-9 hardware (all 9 qubits, 10 edges), ZNE achieves $2.3\times$ mean improvement, with striking position-dependent error: qubit~0 retains 95.1\% magnetization at depth 5, while qubit~8 retains only 2.5\%---consistent with Tuna-9's asymmetric topology where peripheral qubits suffer more decoherence.
This confirms the paper's central claim that ZNE enables useful quantum computation, while revealing that the improvement factor is circuit-depth dependent: $14.1\times$ (emulator noise model) $> 2.3\times$ (Tuna-9) $> 1.3\times$ (IBM with TREX), consistent with our finding that TREX is most effective for shallow circuits where readout error dominates.

% ===================================================================
\section{Failure Taxonomy}
\label{sec:failures}
% ===================================================================

Across all replication attempts, we identified five distinct failure modes:

\begin{enumerate}
  \item \textbf{Noise degradation} (3 failures): Hardware noise exceeds the signal from weak correlators.
  This affected HeH$^+$ VQE on IBM Torino, where $\langle XX \rangle$ and $\langle YY \rangle$ terms in the molecular Hamiltonian contribute $< 0.1\Ha$ but the noise floor is $\sim 0.13\Ha$.
  \emph{Mitigation}: TREX and zero-noise extrapolation can partially address this, but were insufficient for HeH$^+$.

  \item \textbf{Topology constraints} (0 failures, but limited scope): Tuna-9's tree topology prevents replication of experiments requiring all-to-all connectivity (e.g., 3-regular QAOA graphs).
  The agent correctly identified this limitation and restricted tests to topology-compatible subgraphs.

  \item \textbf{Compilation artifacts} (0 failures, but misleading results): Transpiler optimizations can make benchmarking results appear better than the underlying hardware.
  RB fidelity on IBM/IQM was inflated by Clifford compilation; only Tuna-9's unoptimized submission reflected true gate fidelity.

  \item \textbf{Calibration sensitivity} (observed but not causing failure): VQE energy on Tuna-9 varies by $\sim$8$\kcalmol$ between qubit pairs and $\sim$3$\kcalmol$ between runs on the same pair.
  Published papers rarely report this variance.

  \item \textbf{Error mitigation dependency} (1 effective failure): Sagastizabal's chemical accuracy claim required TREX on IBM Torino; without mitigation, the result would fail.
  The paper's symmetry verification protocol achieved $119\times$ error reduction---but only because the IBM platform supports the necessary twirled readout correction.
\end{enumerate}

The dominant failure mode is noise degradation of off-diagonal Hamiltonian terms.
This is consistent with the dephasing noise observed on Tuna-9 and Garnet: $Z$-basis measurements (which probe diagonal Hamiltonian terms) are well-preserved, while $X$ and $Y$ basis measurements (which require additional gates and are sensitive to dephasing) suffer disproportionately.

% ===================================================================
\section{Discussion}
\label{sec:discussion}
% ===================================================================

\subsection{What Replication Gaps Reveal}

The 93\% overall pass rate masks an important asymmetry: all failures occur on hardware, never on the emulator.
This means the AI agent consistently constructs correct circuits and analysis---the bottleneck is hardware noise, not algorithmic understanding.

This has implications for the reproducibility of quantum experiments.
Papers that report results ``within chemical accuracy'' often rely on specific error mitigation techniques, carefully selected qubit pairs, or calibration procedures that are underspecified.
Our H$_2$ replication succeeded on IBM Torino with TREX mitigation ($119\times$ error reduction) and on Tuna-9 through multiple independent approaches: hybrid PS+REM ($0.92\kcalmol$), stacked REM+ZNE(quadratic) ($2.9$\,mHa average across 7 distances), and a true hybrid VQE loop ($0.56\kcalmol$ at equilibrium).
The fact that three distinct mitigation strategies each achieve or approach chemical accuracy on the same hardware demonstrates that Tuna-9 operates in a regime where error mitigation is genuinely effective---not just a statistical fluctuation.
Conversely, HeH$^+$ achieves only $\sim$4.4$\kcalmol$ on both platforms because its Hamiltonian's coefficient amplification ratio ($|g_1|/|g_4| = 7.8$) demands more from the hardware than current noise levels permit.

\subsection{Cross-Platform Insights}

Running identical circuits on three processors reveals systematic differences invisible to single-platform studies:
\begin{itemize}
  \item Tuna-9 and Garnet show \emph{dephasing} noise ($\langle ZZ \rangle \gg \langle XX \rangle \approx |\langle YY \rangle|$), while Torino shows \emph{depolarizing} noise (all correlators approximately equal). This has direct implications for which error mitigation strategies will be effective.
  \item Quantum volume converges at 32 for both Garnet (20~qubits) and Torino (133~qubits), suggesting that QV is dominated by the best local qubit neighborhoods rather than system size.
  \item GHZ per-qubit error is remarkably constant ($\sim$5\%) from 3 to 50 qubits on Torino, indicating that heavy-hex routing introduces minimal crosstalk for linear entanglement chains.
\end{itemize}

\subsection{AI as Replication Infrastructure}

The AI agent's ability to replicate 93\% of tested claims without human intervention suggests that autonomous replication is a viable tool for the quantum computing community.
The agent's workflow---read paper, design circuits, test on emulator, run on hardware, compare to published claims---could be standardized as a ``replication audit'' for quantum publications.

Limitations include: the agent cannot access proprietary calibration data, cannot perform real-time feedback (e.g., mid-circuit measurement on platforms that support it), and relies on published circuit descriptions rather than discovering optimal circuits independently.

\subsection{ZNE Extrapolation Order Matters}

A notable methodological finding: the \emph{order} of Richardson extrapolation in ZNE critically determines its effectiveness.
On Tuna-9, linear ZNE (fold factors 1, 3) \emph{worsens} the VQE result relative to REM alone (8.6 vs.\ 7.4\,mHa), while quadratic ZNE (fold factors 1, 3, 5) achieves 2.9\,mHa---the best automated mitigation result.
This is because Tuna-9's noise response to CZ gate folding is sublinear: tripling the CZ count adds less error than expected, so linear extrapolation overshoots the zero-noise limit.
The quadratic fit captures this curvature.
This finding is underemphasized in the ZNE literature, where linear extrapolation is often the default~\cite{temme2017,li2017}.

The hybrid VQE result (0.9\,mHa) further demonstrates that Tuna-9 operates in a regime where hardware noise is low enough for classical optimizers to find the global minimum using only hardware evaluations.
This is a prerequisite for VQE on problems beyond classical simulability.

% ===================================================================
\section{Conclusion}
\label{sec:conclusion}
% ===================================================================

We have demonstrated that an AI agent can systematically replicate quantum computing experiments across multiple hardware platforms, achieving a 93\% success rate on 27 claims from six landmark papers.
The failures are informative: they arise from coefficient amplification in molecular Hamiltonians that pushes hardware noise beyond signal strength, not from algorithmic errors.
Chemical accuracy was achieved for H$_2$ VQE on both IBM Torino ($0.22\kcalmol$ with TREX) and QI Tuna-9 ($0.56\kcalmol$ via hybrid VQE with REM).

Our in-depth characterization of Tuna-9 yields several notable results.
QV~$= 16$ is certified with high confidence (100 circuits, mean HOF $= 0.757$, 97/100 passing), doubling the initially estimated QV~$= 8$ and demonstrating the importance of systematic qubit selection.
Randomized benchmarking across all 9 qubits reveals a wide spread in gate fidelity (98.64\%--99.96\%), with the best qubits (q6, q7) approaching the $10^{-3}$ error per Clifford threshold.
Most significantly, stacked REM + quadratic ZNE reduces H$_2$ VQE error to 2.9\,mHa average across the full dissociation curve, with 3 of 7 bond distances at chemical accuracy---overturning our earlier finding that ZNE was ineffective on this platform.
The true hybrid VQE loop (COBYLA optimizer calling hardware at each iteration) achieves 0.9\,mHa at equilibrium, the best single-point result and a demonstration that Tuna-9's noise is low enough for hardware-in-the-loop optimization.

The cross-platform comparison---three chips, one test suite---provides a rare apples-to-apples benchmark.
We find that noise character (dephasing vs.\ depolarizing) is a more useful hardware descriptor than headline metrics like quantum volume, particularly for predicting VQE performance.

As quantum hardware continues to improve and AI agents become more capable, we anticipate that AI-driven replication will become a standard tool for validating quantum computing claims---complementing peer review with automated, reproducible, cross-platform verification.

% ===================================================================
\section*{Data Availability}
% ===================================================================

All code, raw data, and analysis scripts are publicly available at \url{https://github.com/JDerekLomas/quantuminspire}. Specific resources:

\begin{itemize}
  \item \textbf{Experiment results} (98 JSON files with raw counts, metadata, and checksums): \url{https://github.com/JDerekLomas/quantuminspire/tree/main/experiments/results}
  \item \textbf{Replication reports} (structured JSON + narrative markdown for each paper): \url{https://github.com/JDerekLomas/quantuminspire/tree/main/research/replication-reports}
  \item \textbf{Tabular dataset} (CSV summaries for cross-platform comparison): \url{https://github.com/JDerekLomas/quantuminspire/tree/main/research/dataset}
  \item \textbf{Replication scripts} (circuit generation + analysis): \url{https://github.com/JDerekLomas/quantuminspire/tree/main/scripts}
  \item \textbf{Molecular Hamiltonians} (canonical coefficients for H$_2$ and HeH$^+$): \url{https://github.com/JDerekLomas/quantuminspire/tree/main/experiments/hamiltonians}
  \item \textbf{Interactive dashboard}: \url{https://quantuminspire.vercel.app}
\end{itemize}

% ===================================================================
\begin{acknowledgments}
This work was supported by QuTech and the Faculty of Industrial Design Engineering at TU Delft.
Hardware access was provided by Quantum Inspire (QuTech/TNO), IBM Quantum, and IQM Resonance.
The entire experimental pipeline---including circuit design, hardware submission, data analysis, and initial manuscript drafting---was performed by Claude Opus 4.6 (Anthropic) operating as an autonomous agent within the Claude Code CLI environment.
The human author directed the research questions, selected target papers, reviewed results, and edited the manuscript.
The AI agent's role is described in detail in Section~\ref{sec:methods}.
\end{acknowledgments}

\bibliography{references}

\end{document}
